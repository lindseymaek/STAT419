# XR[,1] = x * cos(deg2rad(60)) - y * sin(deg2rad(60));
plot(XRs, pch=20, cex=0.25, main="XRs");
abline(v=mean(XRs[,1]), col="green");
abline(h=mean(XRs[,2]), col="green");
points(x=mean(XRs[,1]),y=mean(XRs[,2]),
pch=21, col="green", cex=8);
XRs.PCA = princomp(XRs);
summary(XRs.PCA);
str(XRs.PCA);
summary(XRs.PCA);
str(Xs.PCA);
XRs.PCA.scores = XRs.PCA$scores;
plot(XRs.PCA.scores, pch=20, cex=0.25, main="XRs");
abline(v=mean(XRs.PCA.scores[,1]), col="orange");
abline(h=mean(XRs.PCA.scores[,2]), col="orange");
points(x=mean(XRs.PCA.scores[,1]),y=mean(XRs.PCA.scores[,2]),
pch=21, col="orange", cex=8);
?rmvn
# cache.rebuild ... This will prevent Xs from being used from previous cache ...
library(scatterplot3d);
library(rgl);
library(rgl);
library(rgl)
set.seed(1222015);
mu = c(1,3,8); # centers for x,y
Sigma = diag(c(2,23,13)); # variance for x,y
X = rmvn(nsim, mu, Sigma, ncores=2);  # this is parallelizability with cores
xyz.lim = c(min(X), max(X)); # square
print("################   X   ################");
print(paste0("MEANS:    x = ",round(mean(X[,1]),3),
"       y = ",round(mean(X[,2]),3),
"       z = ",round(mean(X[,3]),3)));
print(paste0("VARIANCE: x = ",round(var(X[,1]),3),
"       y = ",round(var(X[,2]),3),
"       z = ",round(var(X[,3]),3)));
scatterplot3d(X, xlim=xyz.lim, ylim=xyz.lim, zlim=xyz.lim, highlight.3d=FALSE, main="X - 3D Scatterplot", color="red" );
# this is interactive, and will open in its own window
plot3d(X, xlim=xyz.lim, ylim=xyz.lim, zlim=xyz.lim, main="X - 3D plot",col="red" );
install.packages("rgl")
library(rgl);
Xs = scale(X);
print("################   Xs   ################");
print(paste0("MEANS:    x = ",round(mean(Xs[,1]),3),
"       y = ",round(mean(Xs[,2]),3),
"       z = ",round(mean(Xs[,3]),3)));
print(paste0("VARIANCE: x = ",round(var(Xs[,1]),3),
"       y = ",round(var(Xs[,2]),3),
"       z = ",round(var(Xs[,3]),3)));
graphics::plot( as.data.frame(Xs) );
## DROPBOX ... __student_access__ ...
## You need to change this ...
path.dataset = "C:/Users/Alexander Nevsky/Dropbox/WSU-419/Fall 2020/__student_access__/sample_latex_files/Multivariate-2009/datasets/";
file.running = paste0(path.dataset,"RECORDS.csv");
myData = read.csv(file.running,header=FALSE);
## DROPBOX ... __student_access__ ...
## You need to change this ...
path.dataset = "/Users/lindseykornowske/.git/STAT419/datasets/";
file.running = paste0(path.dataset,"RECORDS.csv");
myData = read.csv(file.running,header=FALSE);
colnames(myData)=c("Country","100m","200m","400m",
"800m","1500m","5000m",
"10000m","marathon");
myData;
X = (myData[,-1]);
Xs = scale(X);
rownames(X) = rownames(Xs) = myData[,1]; # We still have the country names, important when we get to biplot
performKMOTest(Xs);
performBartlettSphericityTest(Xs);
source_url( paste0(path.github, "humanVerseWSU/R/functions-str.R") );  # printMatrix
n = nrow(Xs); n;  # number of observations
m = ncol(Xs); m;  # number of features
# why does covariance equal correlation
# it's scaled
# isClose deals with rounding/floating-point issues
print("Comparing covariance and correlation of Xs");
isClose( as.numeric( cov(Xs) ), as.numeric( cor(Xs) ) );
S = ( transposeMatrix(as.matrix(Xs)) %*% as.matrix(Xs)) / (n-1);
dim(S);
print("Comparing S and the covariance of Xs");
isClose( as.numeric( S ), as.numeric( cov(Xs) ) );
S.eigen = eigen(S); # eigenvalues again
str(S.eigen);
Lambda = S.eigen$values;
length(Lambda);   # notice it is of length "m"
# S - Lambda * I, its determinant
round( det(S - diag(Lambda)), digits=4);
D = diag(Lambda);  # Lambda * I
printMatrix(D, 5);
D.sqrt = diag(sqrt(Lambda));
D.sqrt;
printMatrix(D.sqrt, 5);
W = S.eigen$vectors;  # orthogonal unit vectors
dim(W);
T = Xs %*% W;
dim(T);
# The transpose of W is sometimes called the whitening or sphering transformation.
#round( transposeMatrix(W), digits=2);  # https://en.wikipedia.org/wiki/Whitening_transformation
printMatrix( transposeMatrix(W) , 5);
VAF = round(Lambda / traceMatrix(S), digits=4);
VAF.cumsum = cumsum(VAF);
# loadings  ??
F = as.matrix(W) %*% D.sqrt;
dim(F);
Z = ( W %*% D %*% transposeMatrix(W) );  # (n-1) is built into each component ...
dim(Z);
isClose(S, Z);
Answer = round( rbind(F, rep(NA,times=8), Lambda, VAF, VAF.cumsum)
,digits=4);
rownames(Answer) =
c("100m","200m","400m","800m",
"1500m","5000m","10000m","marathon",
"", "EIGEN","% VAF", "C. % VAF");
printMatrix(Answer, 4);
S = ( transposeMatrix(as.matrix(Xs)) %*% as.matrix(Xs)) / (n-1);
dim(S);
print("Comparing S and the covariance of Xs");
isClose( as.numeric( S ), as.numeric( cov(Xs) ) );
S.eigen = eigen(S); # eigenvalues again
str(S.eigen);
Lambda = S.eigen$values;
length(Lambda);   # notice it is of length "m"
# S - Lambda * I, its determinant
round( det(S - diag(Lambda)), digits=4);
D = diag(Lambda);  # Lambda * I
printMatrix(D, 5);
D.sqrt = diag(sqrt(Lambda));
D.sqrt;
printMatrix(D.sqrt, 5);
W = S.eigen$vectors;  # orthogonal unit vectors
dim(W);
T = Xs %*% W;
dim(T);
# The transpose of W is sometimes called the whitening or sphering transformation.
#round( transposeMatrix(W), digits=2);  # https://en.wikipedia.org/wiki/Whitening_transformation
printMatrix( transposeMatrix(W) , 5);
VAF = round(Lambda / traceMatrix(S), digits=4);
VAF.cumsum = cumsum(VAF);
# loadings  ??
F = as.matrix(W) %*% D.sqrt;
dim(F);
Z = ( W %*% D %*% transposeMatrix(W) );  # (n-1) is built into each component ...
dim(Z);
isClose(S, Z);
Xs.princomp = stats::princomp(Xs);
summary(Xs.princomp);   # , loadings=TRUE);
Xs.prcomp = stats::prcomp(Xs);
summary(Xs.prcomp);
Xs.prcomp.E = zeroIsh ( cov(Xs.prcomp$x) );  # better than zapsmall
printMatrix(Xs.prcomp.E, 4);
#Xs.prcomp.E;
Xs.prcomp.lambda = diag(Xs.prcomp.E);
Xs.prcomp.lambda;
# From matrix "maths"  ... Equal except for some rounding errors ...
Lambda;
zeroIsh( Lambda - Xs.prcomp.lambda );
# isClose is a much more precise comparison ... based on floating-point issues
# zeroIsh is just dropping values very close to zero as a form of 1/10^digits of accuracy
Xs.princomp = stats::princomp(Xs);
summary(Xs.princomp);   # , loadings=TRUE);
Xs.princomp = stats::princomp(Xs);
summary(Xs.princomp);   # , loadings=TRUE);
Xs.prcomp = stats::prcomp(Xs);
summary(Xs.prcomp);
Xs.prcomp.E = zeroIsh ( cov(Xs.prcomp$x) );  # better than zapsmall
printMatrix(Xs.prcomp.E, 4);
Xs.prcomp.lambda = diag(Xs.prcomp.E);
Xs.prcomp.lambda;
# From matrix "maths"  ... Equal except for some rounding errors ...
Lambda;
zeroIsh( Lambda - Xs.prcomp.lambda );
source_url( paste0(path.github, "humanVerseWSU/R/functions-EDA.R") );   # how many factors
Xs.how.many = howManyFactorsToSelect(Xs);
dim(Xs);  # data
dim(S);   # sample variance (correlation if Xs)
dim(W);   # the eigenvectors associated with
length(Lambda);
n;
m;
dim(T);
# what do you notice ??? ... orthogonality
zapsmall( W %*% transposeMatrix(W), digits=2);
zapsmall( transposeMatrix(W) %*% W, digits=2);
zeroIsh( W %*% transposeMatrix(W) );
zeroIsh( transposeMatrix(W) %*% W );
# what do you notice ??? ... orthogonality
zapsmall( W %*% transposeMatrix(W), digits=2);
zapsmall( transposeMatrix(W) %*% W, digits=2);
zeroIsh( W %*% transposeMatrix(W) );
zeroIsh( transposeMatrix(W) %*% W );
# what do you notice ??? ... orthogonality
zapsmall( W %*% transposeMatrix(W), digits=2);
zapsmall( transposeMatrix(W) %*% W, digits=2);
zeroIsh( W %*% transposeMatrix(W) );
zeroIsh( transposeMatrix(W) %*% W );
biplot(Xs.princomp);  # equivalent to # biplot(Xs.prcomp);
biplot(Xs.princomp, 1:2);
biplot(Xs.princomp, 3:2);  # Netherlands and something with 200 meters
biplot(Xs.princomp, 3:4);
biplot(Xs.princomp, 5:4);  # Cook & marathon, Mauritius and 100m, W. Samoa and 800m
biplot(Xs.princomp, 5:6);
biplot(Xs.princomp, 7:6);
biplot(Xs.princomp, 7:8);
X[c(12,34,36,38,51,55,53,54),];
source_url( paste0(path.github, "humanVerseWSU/R/functions-stats.R") );  # updated findOutliersUsingIQR
## so let's examine
Countries = rownames(X);
Events = colnames(X);
for(i in 1:8)
for(i in 1:8)
{
event = Events[i];
print("############################");
print(paste0( " Event :: ", event ) );  # print(paste0 ... should be a helper function ... use it all the time
print("############################");
outliers = findOutliersUsingIQR(X[,i]);
very.slow = outliers$outer.upper;  # outer fence ...
slow = setdiff(outliers$inner.upper, very.slow);  # "unique" inner fence ...
very.fast = outliers$outer.lower;  # outer fence ...
fast = setdiff(outliers$inner.lower,very.fast);  # inner fence ...
if(length(very.slow) > 0)
{
print("-------------- very.slow --------------");
print( Countries[ very.slow ] );
print("---------------------------------------");
}
if(length(slow) > 0)
{
print("---------------- slow -----------------");
print( Countries[ slow ] );
print("---------------------------------------");
}
if(length(fast) > 0)
{
print("---------------- fast -----------------");
print( Countries[ fast ] );
print("---------------------------------------");
}
if(length(very.fast) > 0)
{
print("-------------- very.fast --------------");
print( Countries[ very.fast ] );
print("---------------------------------------");
}
}
source_url( paste0(path.github, "humanVerseWSU/R/functions-EDA.R") );
# Xs;
dim(Xs);
Xs.hclust.8 = perform.hclust(Xs, n.groups = 8, pvclust.parallel = TRUE);
# Xs;
dim(Xs);
Xs.hclust.8 = perform.hclust(Xs, n.groups = 8, pvclust.parallel = TRUE);
Xs.t = transposeMatrix(Xs);
#Xs.t;
dim(Xs.t);
Xs.t.hclust.6 = perform.hclust(Xs.t, n.groups = 6, pvclust.parallel = TRUE);
source_url( paste0(path.github, "humanVerseWSU/R/functions-EDA.R") );
# Xs;
dim(Xs);
Xs.hclust.8 = perform.hclust(Xs, n.groups = 8, pvclust.parallel = TRUE);
View(perform.hclust)
View(perform.hclust)
source_url( paste0(path.github, "humanVerseWSU/R/functions-EDA.R") );
# Xs;
dim(Xs);
Xs.hclust.8 = perform.hclust(Xs, n.groups = 8, pvclust.parallel = F);
Xs.t.hclust.2 = perform.hclust(Xs.t, n.groups = 2, pvclust.parallel = TRUE);
# cache.rebuild ... This will prevent Xs from being used from previous cache ...
library(mvnfast);
source_url( paste0(path.github, "humanVerseWSU/R/functions-maths.R") );   # deg2rad
# zeroIsh
set.seed(1222015);
mu = c(1,3); # centers for x,y
Sigma = diag(c(2,23)); # variance for x,y
nsim = 9000;
X = rmvn(nsim, mu, Sigma, ncores=2);  # this is parallelizability with cores
# ncores	... Number of cores used. The parallelization will take place only if OpenMP is supported.
xy.lim = c(min(X), max(X)); # square
print("################   X   ################");
print(paste0("MEANS:    x = ",round(mean(X[,1]),3),
"       y = ",round(mean(X[,2]),3) ));
print(paste0("VARIANCE: x = ",round(var(X[,1]),3),
"       y = ",round(var(X[,2]),3) ));
plot(X, pch=20, cex=0.25, main="X",
xlim=xy.lim, ylim=xy.lim );
abline(v=mean(X[,1]), col="red");
abline(h=mean(X[,2]), col="red");
points(x=mean(X[,1]),y=mean(X[,2]),
pch=21, col="red", cex=8);
Xs = scale(X);
print("################   Xs   ################");
print(paste0("MEANS:    x = ",round(mean(Xs[,1]),3),
"       y = ",round(mean(Xs[,2]),3) ));
print(paste0("VARIANCE: x = ",round(var(Xs[,1]),3),
"       y = ",round(var(Xs[,2]),3) ));
plot(Xs, pch=20, cex=0.25, main="Xs",
xlim=xy.lim, ylim=xy.lim );
abline(v=mean(Xs[,1]), col="blue");
abline(h=mean(Xs[,2]), col="blue");
points(x=mean(Xs[,1]),y=mean(Xs[,2]),
pch=21, col="blue", cex=8);
plot(Xs, pch=20, cex=0.25, main="Xs");
abline(v=mean(Xs[,1]), col="green");
abline(h=mean(Xs[,2]), col="green");
points(x=mean(Xs[,1]),y=mean(Xs[,2]),
pch=21, col="green", cex=8);
source_url( paste0(path.github, "humanVerseWSU/R/functions-str.R") );  # printMatrix
n = nrow(Xs); n;  # number of observations
m = ncol(Xs); m;  # number of features
Sigma = ( transposeMatrix(as.matrix(Xs)) %*% as.matrix(Xs)) ;
Sigma.eigen = eigen(Sigma); # eigenvalues again
Lambda = Sigma.eigen$values;
D = diag(Lambda);  # Lambda * I
D.sqrt = diag(sqrt(Lambda));
T = Sigma.eigen$vectors;  # orthogonal unit vectors
W = Xs %*% T;
F = as.matrix(W) %*% D.sqrt;
Z = ( W %*% D %*% transposeMatrix(W) );
n = nrow(Xs); n;  # number of observations
m = ncol(Xs); m;  # number of features
Sigma = ( transposeMatrix(as.matrix(Xs)) %*% as.matrix(Xs)) ;
Sigma.eigen = eigen(Sigma); # eigenvalues again
Lambda = Sigma.eigen$values;
D = diag(Lambda);  # Lambda * I
D.sqrt = diag(sqrt(Lambda));
T = Sigma.eigen$vectors;  # orthogonal unit vectors
W = Xs %*% T;
F = as.matrix(W) %*% D.sqrt;
Z = ( W %*% D %*% transposeMatrix(W) );
# returning the matrices U and V from svd are not super-efficient, unless of course you need them...
p = m;  # you could have it "data-reduce" and only report a number smaller than the total features "m"
Xs.svd = svd(Xs, nu = p, nv = p);
D.svd = Xs.svd$d;  D.svd;
U.svd = Xs.svd$u;  dim(U.svd);
V.svd = Xs.svd$v;  dim(V.svd);  V.svd
print("Are D.pca and D.svd equivalent (when scaled)?");
D.pca = sqrt( diag(D) );  D.pca;
isClose(D.pca, D.svd);
print("Are W.pca and U.svd equivalent (when scaled)?");
W.pca = W / D.pca;  dim(W.pca);  # scaling factor is required ...
my.diff = zeroIsh( (W.pca - U.svd), 5);  # the SVD technique is numerical, not exactly zero-Ish ... but look at the summary ...
summary(my.diff);
print("Are T.pca and V.svd equivalent (sign-scaling required)?");
T.pca = T;  # this is why we need to spell out TRUE ...
isClose(T.pca, V.svd);
# we have a sign-change on col #2
isClose(T.pca[,1], V.svd[,1]);
isClose(T.pca[,2], -1*V.svd[,2]);  #
mySetSeed = 1222015;
#option.n = c(10^1, 10^2, 10^3, 10^4);
#option.m = c(5^1, 5^2, 5^3, 5^4, 5^5, 5^6);
option.n = c(1000, 3000, 9000);
option.m = c(25, 125, 625);
timing.m = c();
timing.n = c();
timing.rmvn = c();
timing.cov = c();
timing.xxt = c();
timing.pca = c();
timing.svd.slow = c();
timing.svd.fast = c();
for(m in option.m)
{
for(n in option.n)
{
timing.m = c(timing.m, m);
timing.n = c(timing.n, n);
print(paste0(" SIMULATION ===>   m :: ",m,"    n :: ",n));
################### GENERATION rmvn ################
print("  ------------------------> rmvn ");
time.start = Sys.time();
# we will let this be random every time ...
mu = round( runif(m, 1, 5), digits = 0);
# we will let this be random every time ...
Sigma = diag( round( runif(m, 1, 25), digits = 0) );
set.seed(mySetSeed);  # the next line will run the same every time (if mu and Sigma are the same) ...
X = rmvn(n, mu, Sigma, ncores=2);
Xs = scale(X);
time.end = Sys.time();
elapse = sprintf("%.3f", as.numeric(time.end) - as.numeric(time.start));
timing.rmvn = c(timing.rmvn, elapse);
print(elapse);
################### cov(X) ################
print("  ------------------------> cov(X) ");
time.start = Sys.time();
Xs.cov = cov(Xs);
time.end = Sys.time();
elapse = sprintf("%.3f", as.numeric(time.end) - as.numeric(time.start));
timing.cov = c(timing.cov, elapse);
print(elapse);
################### X %*% transposeMatrix( X ) #####
print("  ------------------------> matrix computations ");
time.start = Sys.time();
n = nrow(Xs); n;  # number of observations
m = ncol(Xs); m;  # number of features
Sigma = ( transposeMatrix(as.matrix(Xs)) %*% as.matrix(Xs)) ;
Sigma.eigen = eigen(Sigma); # eigenvalues again
Lambda = Sigma.eigen$values;
D = diag(Lambda);  # Lambda * I
D.sqrt = diag(sqrt(Lambda));
T = Sigma.eigen$vectors;  # orthogonal unit vectors
W = Xs %*% T;
F = as.matrix(W) %*% D.sqrt;
Z = ( W %*% D %*% transposeMatrix(W) );
time.end = Sys.time();
elapse = sprintf("%.3f", as.numeric(time.end) - as.numeric(time.start));
timing.xxt = c(timing.xxt, elapse);
print(elapse);
################### PCA princomp #####
print("  ------------------------> PCA princomp ");
time.start = Sys.time();
if(n > m)
{
Xs.princomp = stats::princomp(Xs);
}
time.end = Sys.time();
elapse = sprintf("%.3f", as.numeric(time.end) - as.numeric(time.start));
timing.pca = c(timing.pca, elapse);
print(elapse);
################### SVD full   #####
print("  ------------------------> SVD full return ");
time.start = Sys.time();
p = m;  # you could have it "data-reduce" and only report a number smaller than the total features "m"
Xs.svd = svd(Xs, nu = p, nv = p);
time.end = Sys.time();
elapse = sprintf("%.3f", as.numeric(time.end) - as.numeric(time.start));
timing.svd.slow = c(timing.svd.slow, elapse);
print(elapse);
################### SVD fast   #####
print("  ------------------------> SVD fast return ");
time.start = Sys.time();
p = 0;  # you could have it "data-reduce" and only report a number smaller than the total features "m"
Xs.svd = svd(Xs, nu = p, nv = p);
time.end = Sys.time();
elapse = sprintf("%.3f", as.numeric(time.end) - as.numeric(time.start));
timing.svd.fast = c(timing.svd.fast, elapse);
print(elapse);
}
}
# cbind on vectors ...
my.times = cbind(timing.n, timing.m, timing.rmvn,
timing.cov, timing.xxt, timing.pca,
timing.svd.slow, timing.svd.fast);
colnames(my.times) = c("n", "m", "rmvn", "cov", "xxt", "pca", "svd.slow", "svd.fast");
my.times = as.data.frame(my.times);
my.times;
my.times = structure(list(n = c("1000", "3000", "9000", "1000", "3000",
"9000", "1000", "3000", "9000"), m = c("25", "25", "25", "125",
"125", "125", "625", "625", "625"), rmvn = c("0.003", "0.006",
"0.021", "0.010", "0.034", "0.100", "0.146", "0.517", "1.366"
), cov = c("0.000", "0.001", "0.004", "0.007", "0.023", "0.068",
"0.189", "0.537", "1.642"), xxt = c("0.015", "0.115", "1.060",
"0.093", "0.610", "5.459", "1.307", "6.077", "38.488"), pca = c("0.003",
"0.007", "0.020", "0.026", "0.160", "0.229", "0.640", "1.600",
"4.791"), svd.slow = c("0.002", "0.005", "0.012", "0.034", "0.093",
"0.319", "0.992", "2.810", "7.517"), svd.fast = c("0.001", "0.002",
"0.006", "0.014", "0.036", "0.121", "0.383", "0.983", "2.669"
)), class = "data.frame", row.names = c(NA, -9L));
# cbind on vectors ...
my.times = cbind(timing.n, timing.m, timing.rmvn,
timing.cov, timing.xxt, timing.pca,
timing.svd.slow, timing.svd.fast);
colnames(my.times) = c("n", "m", "rmvn", "cov", "xxt", "pca", "svd.slow", "svd.fast");
my.times = as.data.frame(my.times);
my.times;
# cbind on vectors ...
my.times = cbind(timing.n, timing.m, timing.rmvn,
timing.cov, timing.xxt, timing.pca,
timing.svd.slow, timing.svd.fast);
colnames(my.times) = c("n", "m", "rmvn", "cov", "xxt", "pca", "svd.slow", "svd.fast");
my.times = as.data.frame(my.times);
my.times;
plot(my.times)
knitr::opts_chunk$set(echo = TRUE);
knitr::opts_chunk$set(warning = FALSE);
knitr::opts_chunk$set(message = FALSE);
## this should knit, but I am running some IMDB stuff
## so I wasn't able to verify a final Knit.
## please let me know in the Discussion Board if you
## find any errors, and I will fix
# we don't want scientific notation
options(scipen  = 999);
library(devtools);
library(humanVerseWSU);
packageVersion("humanVerseWSU");  # should be ‘0.1.4’+
library(rvest);
library(magrittr);
# how long after commit/push before "raw" becomes live?
# unraw was immediate ...  3:50 AM ... about 7 minutes https://github.com/MonteShaffer/humanVerseWSU/blob/master/humanVerseWSU/R/functions-cleanup.R
# sometimes it is immediate ...
path.github = "https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/";
# a few DMS <--> DEC conversion tools for lat/long ... in the end, I didn't need them ...
# source("C:/_git_/MonteShaffer/humanVerseWSU/humanVerseWSU/R/functions-maths.R");
## I reorganized the `grabHTML` function that is the key driver of the caching system ... It is not compiled yet, so grab it from its source_url
source_url( paste0(path.github, "humanVerseWSU/R/functions-file.R") );
## I may never directly wrap them into the library ... harvesting/parsing are tasks/processes that need to be performed and documented, that does not mean they belong as "public functions" in the library
source_url( paste0(path.github, "humanVerseWSU/R/functions-wikipedia.R") );
# UTF encoding issues ... notice the degree symbol, the single quote that is not really, and the double-quote that is not really...
# wiki.cleanupDMStoDecLatitudeLongitude("48°22′13″N 114°11′20″W");
# the above function can be found in humanVerseWSU/compiling/_stuff_/ just as an archive ...
local.data.path = "R:/";  # to store HTML files, cached data-objects
# I generally use large drives because they fill up
# Having the C:/ run out can be bad
# Today's project [~6MB zipped]
path.wiki = paste0(local.data.path,"Wikipedia/");
createDirRecursive(path.wiki);
wiki.url = "https://en.wikipedia.org/wiki/Columbia_Falls,_Montana";
wiki.info = wiki.downloadWikiPage(wiki.url, path.wiki);
wiki.url = "https://en.wikipedia.org/wiki/Columbia_Falls,_Montana";
wiki.info = wiki.downloadWikiPage(wiki.url, path.wiki);
latlong = wiki.findCoordinates(wiki.info$path.wiki.page, wiki.info$wiki.html);
wiki.url = "https://en.wikipedia.org/wiki/Whitefish,_Montana";
wiki.info = wiki.downloadWikiPage(wiki.url, path.wiki);
