colors = rainbow(50, s = 0.6, v = 0.75); # 50 colors for 50 states
# descriptive star plot to start
stars(whichX, len = 0.5, key.loc=c(12,2), draw.segments = TRUE);
## too busy, let's group them
x.start = 1;
x.end = 10;
for(i in 1:5)
{
stars( whichX[x.start:x.end,] ,
len = 0.5, key.loc=c(6,2), draw.segments = TRUE);
x.start = 1 + x.end;
x.end = 10 + x.end;
}
k = 12;
iterations = 100;
number.starts = 100;
whichX.kmeans = kmeans(whichX, 12,
iter.max=iterations,
nstart = number.starts);  # default algorithm
whichX = X;
k = 12;
iterations = 100;
number.starts = 100;
whichX.kmeans = kmeans(whichX, 12,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
membership = matrix( whichX.kmeans$cluster, ncol=1);
membership = membership[order(membership),];
membership = as.data.frame(membership);
rownames(membership) = climate.df$labels;
colnames(membership) = c("Cluster");
membership;
print( table(membership) ) ;
# I believe in an older version of R these were called $centroids
attributes = as.data.frame( whichX.kmeans$centers );
rownames(attributes) = paste0("Cluster.",1:12);
attributes;
k = 5;
iterations = 100;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, 12,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
k = 5;
iterations = 100;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
k = 6;
iterations = 100;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
k = 4;
iterations = 100;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
# 4 clusters seems appropriate - when run with more, clusters 5 and 6 were similar to cluster 3
k = 4;
iterations = 1000;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
# 4 clusters seems appropriate - when run with more, clusters 5 and 6 were similar to cluster 3
k = 4;
iterations = 100;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
# 4 clusters seems appropriate - when run with more, clusters 5 and 6 were similar to cluster 3
k = 4;
iterations = 10000;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
# 4 clusters seems appropriate - when run with more, clusters 5 and 6 were similar to cluster 3
k = 6;
iterations = 10000;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
# 4 clusters seems appropriate - when run with more, clusters 5 and 6 were similar to cluster 3
k = 5;
iterations = 10000;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
# 4 clusters seems appropriate - when run with more, clusters 5 and 6 were similar to cluster 3
k = 4;
iterations = 10000;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
membership = matrix( whichX.kmeans$cluster, ncol=1);
membership = membership[order(membership),];
membership = as.data.frame(membership);
rownames(membership) = climate.df$labels;
colnames(membership) = c("Cluster");
membership;
print( table(membership) ) ;
# I believe in an older version of R these were called $centroids
attributes = as.data.frame( whichX.kmeans$centers );
rownames(attributes) = paste0("Cluster.",1:12);
membership = matrix( whichX.kmeans$cluster, ncol=1);
membership = membership[order(membership),];
membership = as.data.frame(membership);
rownames(membership) = climate.df$labels;
colnames(membership) = c("Cluster");
membership;
print( table(membership) ) ;
# I believe in an older version of R these were called $centroids
attributes = as.data.frame( whichX.kmeans$centers );
rownames(attributes) = paste0("Cluster.",1:k);
attributes;
# 4 clusters seems appropriate - when run with more, clusters 5 and 6 were similar to cluster 3
k = 12;
iterations = 10000;
number.starts = 100;
## I could not get this to run on the scaled data - whichX is unscaled
whichX.kmeans = kmeans(whichX, k,
iter.max=iterations,
nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);
membership = matrix( whichX.kmeans$cluster, ncol=1);
membership = membership[order(membership),];
membership = as.data.frame(membership);
rownames(membership) = climate.df$labels;
colnames(membership) = c("Cluster");
membership;
print( table(membership) ) ;
# I believe in an older version of R these were called $centroids
attributes = as.data.frame( whichX.kmeans$centers );
rownames(attributes) = paste0("Cluster.",1:k);
attributes;
library(Hmisc); # p-values for correlation
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
high.X = high[,c(5:18,21)]; # numeric data
high.cor = round( cor(high.X), digits=2);
str(high.X)
library(Hmisc); # p-values for correlation
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
high.X = as.numeric(high[,c(5:18,21)]); # numeric data
View(high)
library(Hmisc); # p-values for correlation
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
high.X = (high[,c(5:16)]; # numeric data #lat/long were reading as factors
library(Hmisc); # p-values for correlation
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
high.X = (high[,c(5:16)] # numeric data #lat/long were reading as factors
high.cor = round( cor(high.X), digits=2);
library(Hmisc); # p-values for correlation
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
high.X = (high[,c(5:16)]); # numeric data #lat/long were reading as factors
high.cor = round( cor(high.X), digits=2);
# high.cor.p = rcorr(as.matrix(high.X), type="pearson");  # p-values for statistical significance ... # str(high.cor.p);
# examine July (idx = 7)
as.data.frame( high.cor ) ; # so it will render nicely in RStudio
high.cor.july = high.cor[,7];
high.cor.july;
View(high)
library(Hmisc); # p-values for correlation
low = subsetDataFrame(climate, c("key", "units"), "==", c("Record low F (C)",1));
low = merge(low, capitals, by=c("capital","state"));
low.X = low[,c(5:18,21)]; # numeric data
low.cor = round( cor(low.X), digits=2);
library(Hmisc); # p-values for correlation
low = subsetDataFrame(climate, c("key", "units"), "==", c("Record low F (C)",1));
low = merge(low, capitals, by=c("capital","state"));
low.X = low[,c(5:16)]; # numeric data
low.cor = round( cor(low.X), digits=2);
# low.cor.p = rcorr(as.matrix(low.X), type="pearson");  # p-values for statistical significance ... # str(low.cor.p);
# examine Jan (idx = 1)
as.data.frame( low.cor ) ; # so it will render nicely in RStudio
low.cor.january = low.cor[,1];
low.cor.january;
plot(high.cor.july)
str(climate)
str(capitals)
str(high)
high.X = (high[,c(5:18, 21)]); # numeric data #lat/long were reading as factors, selected only temps
str(high.X)
View(high.X)
numeric(high.X)
double(high.X)
as.double(high.X)
as.character(high.X)
as.numeric(high.X)
View(high.X)
high.cor = round( cor(high.X), digits=2);
as.numeric(capitals)
capitals
View(high.X)
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
capitals
high.X = (high[,c(5:18, 21)]); # numeric data #lat/long were reading as factors, selected only temps
high.cor = round( cor(high.X), digits=2);
high.X.num = as.numeric(paste(high.X))
high.cor = round( cor(high.X), digits=2);
high.cor = round( cor(high.X.num), digits=2);
high.X.num = as.numeric(levels(high.X)[high.X])
high.cor = round( cor(high.X.num), digits=2);
high.X = (high[,c(5:18, 21)]); # numeric data #lat/long were reading as factors, selected only temps
high.X.num = as.numeric(levels(high.X)[high.X])
high.X$latitude = as.numeric(high.X$latitude)
high.X$longitude = as.numeric(high.X$longitude)
high.X$population.2019.est = as.numeric(high.X$population.2019.est);
high.cor = round( cor(high.X.num), digits=2);
library(Hmisc); # p-values for correlation
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
high.X = (high[,c(5:18, 21)]); # numeric data #lat/long were reading as factors, selected only temps
high.X$latitude = as.numeric(high.X$latitude);
high.X$longitude = as.numeric(high.X$longitude);
high.X$population.2019.est = as.numeric(high.X$population.2019.est);
high.cor = round( cor(high.X), digits=2);
# high.cor.p = rcorr(as.matrix(high.X), type="pearson");  # p-values for statistical significance ... # str(high.cor.p);
# examine July (idx = 7)
as.data.frame( high.cor ) ; # so it will render nicely in RStudio
high.cor.july = high.cor[,7];
high.cor.july;
library(reshape2);
View(high.cor)
plot(high.cor)
plot(high.cor.july);
plot(high.cor.july[,c(1:12)]);
plot(high.cor.july[c(1:12)]);
plot(high.cor.july[c(1:12)], xlab = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"));
library(Hmisc); # p-values for correlation
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
high.X = (high[,c(5:18, 21)]); # numeric data #lat/long were reading as factors, selected only temps
high.X$latitude = as.numeric(high.X$latitude);
high.X$longitude = as.numeric(high.X$longitude);
high.X$population.2019.est = as.numeric(high.X$population.2019.est);
high.cor = round( cor(high.X), digits=2);
# high.cor.p = rcorr(as.matrix(high.X), type="pearson");  # p-values for statistical significance ... # str(high.cor.p);
# examine July (idx = 7)
as.data.frame( high.cor ) ; # so it will render nicely in RStudio
high.cor.july = high.cor[,7];
high.cor.july;
plot(high.cor.july[c(1:12)]) axis(1, at = 1:12, labels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"));
library(Hmisc); # p-values for correlation
high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));
high.X = (high[,c(5:18, 21)]); # numeric data #lat/long were reading as factors, selected only temps
high.X$latitude = as.numeric(high.X$latitude);
high.X$longitude = as.numeric(high.X$longitude);
high.X$population.2019.est = as.numeric(high.X$population.2019.est);
high.cor = round( cor(high.X), digits=2);
# high.cor.p = rcorr(as.matrix(high.X), type="pearson");  # p-values for statistical significance ... # str(high.cor.p);
# examine July (idx = 7)
as.data.frame( high.cor ) ; # so it will render nicely in RStudio
high.cor.july = high.cor[,7];
high.cor.july;
plot(high.cor.july[c(1:12)]);
axis(1, at = 1:12, labels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"));
plot(high.cor.july[c(1:12)], xact = "n");
axis(1, at = 1:12, labels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"));
plot(high.cor.july[c(1:12)], xlab = "n");
plot(high.cor.july[c(1:12)],xaxt = "n");
plot(high.cor.july[c(1:12)],xaxt = "n");
axis(1, at = 1:12, labels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"));
plot(high.cor.july[c(1:12)],xaxt = "n");
axis(1, at = 1:12, labels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"));
abline(h=0.3)
plot(high.cor.july[c(1:12)],xaxt = "n");
axis(1, at = 1:12, labels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"));
abline(h=0.3)
abline(h=0.8)
low = subsetDataFrame(climate, c("key", "units"), "==", c("Record low F (C)",1));
low = merge(low, capitals, by=c("capital","state"));
low.X = (low[,c(5:18, 21)]); # numeric data
low.X$latitude = as.numeric(low.X$latitude);
low.X$longitude = as.numeric(low.X$longitude);
low.X$population.2019.est = as.numeric(low.X$population.2019.est);
low.cor = round( cor(low.X), digits=2);
as.data.frame( low.cor ) ; # so it will render nicely in RStudio
low.cor.january = low.cor[,1];
low.cor.january;
setwd("~/.git/STAT419/Midterm")
# I am now setting parameters in YAML header, look above
knitr::opts_chunk$set(echo = params$knitChunkSetEcho);
knitr::opts_chunk$set(warning = params$knitChunkSetWarning);
knitr::opts_chunk$set(message = params$knitChunkSetMessage);
# ... just added ... take a look at how this builds ... you now have your raw files ...
knitr::opts_chunk$set(cache = params$knitChunkSetCache);
knitr::opts_chunk$set(fig.path = params$knitChunkSetFigPath);
###########################
options(scipen  = 999);
library(humanVerseWSU);
packageVersion("humanVerseWSU");  # ‘0.1.4.2’+
path.github = "https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/";
path.mshaffer = "http://md5.mshaffer.com/WSU_STATS419/";
source( paste0(path.github,"misc/functions-midterm-F2000.R") );  # should be 2020 ... oh well
source( paste0(path.github,"humanVerseWSU/R/functions-EDA.R") );  # EDA functions ...
library(parallel);
parallel::detectCores(); # 16 # Technically, this is threads, I have an 8-core processor
deep.dive = c("Microsoft Office", "C++", "SQL", "Computer Science", "Python", "Java", "Statistics", "Data analysis", "Data analytics", "Javascript", "machine learning", "Git", "Tableau", "Business intelligence", "PHP", "Mysql", "MariaDB", "SAS", "SPSS", "Stata",  "Data entry", "Big data", "Data science", "Power BI");  # I intentionally do not include "R" because it is return irrelevant results, I should have searched for "R programming" or "R statistics" ... which I am now doing...
#or = "";
#for(search in deep.dive)
#  {
# or = paste0(or, " jobs$search.query == '",search,"' | ");
#  }
#or = substr(or,0, strlen(or) - 2);
## TODO ... update subsetDataFrame to allow "OR" logic, currently only does "AND" ...
# jobs.subset = jobs[ or , ];  # doesn't work ...
jobs.subset = jobs[ jobs$search.query == 'Microsoft Office' |  jobs$search.query == 'C++' |  jobs$search.query == 'SQL' |  jobs$search.query == 'Computer Science' |  jobs$search.query == 'Python' |  jobs$search.query == 'Java' |  jobs$search.query == 'Statistics' |  jobs$search.query == 'Data analysis' |  jobs$search.query == 'Data analytics' |  jobs$search.query == 'Javascript' |  jobs$search.query == 'machine learning' |  jobs$search.query == 'Git' |  jobs$search.query == 'Tableau' |  jobs$search.query == 'Business intelligence' |  jobs$search.query == 'PHP' |  jobs$search.query == 'Mysql' |  jobs$search.query == 'MariaDB' |  jobs$search.query == 'SAS' |  jobs$search.query == 'SPSS' |  jobs$search.query == 'Stata' |  jobs$search.query == 'Data entry' |  jobs$search.query == 'Big data' |  jobs$search.query == 'Data science' |  jobs$search.query == 'Power BI'  , ];
# stem(jobs.subset$job.count);
# subsetDataFrame(jobs.subset, "job.count", "==", 0);
latlong = removeAllColumnsBut(capitals,c( "state", "st", "capital", "latitude", "longitude", "population.2019.est") );
# first two elements have to be this
latlong = moveColumnsInDataFrame(latlong, c("longitude","latitude"), "before", "state");
str(latlong)
latlong$longitude = as.numeric(latlong$longitude);
latlong$latitude = as.numeric(latlong$latitude);
# for transform to work
library(usmap);
library(ggplot2);
latlong.transform = usmap_transform(data = latlong);
### plot_usmap ...
plot_usmap(fill = "#53565A", alpha = 0.25) +
ggrepel::geom_label_repel(data = latlong.transform,
aes(x = longitude.1, y = latitude.1, label = capital),
size = 3, alpha = 0.8,
label.r = unit(0.5, "lines"), label.size = 0.5,
segment.color = "#981E32", segment.size = 1,
seed = 1002) +
scale_size_continuous(range = c(1, 16),
label = scales::comma) +
labs(title = "U.S. State Capitals",
subtitle = "Source: Wikipedia (October 2020)") +
theme(legend.position = "right")
library(devtools);
source( paste0(path.mshaffer, "will") );
source( paste0(path.mshaffer, "denzel") );
movies.50 = rbind(will$movies.50, denzel$movies.50);
unique(movies.50$ttid); # are they in any shared movies ???
loadInflationData();
movies.50 = standardizeDollarsInDataFrame(movies.50,
2000,
"millions",
"year",
"millionsAdj2000");
movies.50$cluster.arbitrary = NA;
str(movies.50);
## you do something here ...
# (1) populate cluster.arbitrary
# (2) summarize how many movies live in each (table count)
View(movies.50)
View(movies.50)
library(tidyverse)
# (1) populate cluster.arbitrary
## cluster by audience rated 1 -not rated 2 - is PG 3 - PG-13 4 - R
movies.50 %>% filter(rated == "Not Rated") %>% summarize(cluster.arbitrary = "1");
View(movies.50)
# (1) populate cluster.arbitrary
## cluster by audience rated 1 -not rated 2 - is PG 3 - PG-13 4 - R
movies.50 %>% filter(rated == "Not Rated") %>% summarize(cluster.arbitrary = "1");
View(movies.50)
# (1) populate cluster.arbitrary
## cluster by audience rated 1 -not rated 2 - is PG 3 - PG-13 4 - R
movies.50 %>% filter(rated == "Not Rated") %>% summarize(cluster.arbitrary == "1");
View(movies.50)
# (1) populate cluster.arbitrary
## cluster by audience rated 1 -not rated 2 - is PG 3 - PG-13 4 - R
movies.50 %>% filter(rated == "Not Rated") %>% summarize(cluster.arbitrary == 1);
View(movies.50)
# (1) populate cluster.arbitrary
## cluster by audience rated 1 -not rated 2 - is PG 3 - PG-13 4 - R
movies.50 %>% filter(rated == "Not Rated") %>% summarize(.funs = cluster.arbitrary = 1);
# (1) populate cluster.arbitrary
## cluster by audience rated 1 -not rated 2 - is PG 3 - PG-13 4 - R
movies.50 %>% filter(rated == "Not Rated") %>% cluster.arbitrary = 1;
# (1) populate cluster.arbitrary
## cluster by audience rated 1 -not rated 2 - is PG 3 - PG-13 4 - R
movies.50.1 = movies.50 %>% filter(rated == "Not Rated");
movies.50.1$cluster.arbitrary = 1;
View(movies.50.1)
movies.50.2 = movies.50 %>% filter(rated == "PG");
movies.50.2$cluster.arbitrary = 2;
movies.50.3 = movies.50 %>% filter(rated == "PG-13");
movies.50.4 = movies.50 %>% filter(rated == "R");
movies.50.4$cluster.arbitrary = 4;
movies.50 = data.frame(movies.50.1, movies.50.2, movies.50.3, movies.50.4);
movies.50 = as.data.frame(movies.50.1, movies.50.2, movies.50.3, movies.50.4);
View(movies.50.1)
View(movies.50.2)
View(movies.50.3)
View(movies.50.2)
View(movies.50.1)
View(movies.50.2)
movies.50 = as.data.frame(rbind(movies.50.1, movies.50.2, movies.50.3, movies.50.4));
View(movies.50)
movies.50.3 = movies.50 %>% filter(rated == "PG-13");
movies.50.3$cluster.arbitrary = 3;
movies.50.4 = movies.50 %>% filter(rated == "R");
movies.50.4$cluster.arbitrary = 4;
movies.50 = as.data.frame(rbind(movies.50.1, movies.50.2, movies.50.3, movies.50.4));
View(movies.50)
# (2) summarize how many movies live in each (table count)
rows(movies.50$cluster.arbitrary)
# (2) summarize how many movies live in each (table count)
rowcount(movies.50$cluster.arbitrary)
# (2) summarize how many movies live in each (table count)
row_count(movies.50$cluster.arbitrary)
# (2) summarize how many movies live in each (table count)
length(movies.50$cluster.arbitrary)
# (2) summarize how many movies live in each (table count)
summary(movies.50$cluster.arbitrary)
# (2) summarize how many movies live in each (table count)
count(movies.50$cluster.arbitrary)
counts = (length(movies.50.1))
counts
counts = (length(movies.50.1$cluster.arbitrary),length(movies.50.2$cluster.arbitrary), length(movies.50.3$cluster.arbitrary), length(movies.50.4$cluster.arbitrary));
counts = c(length(movies.50.1$cluster.arbitrary), length(movies.50.2$cluster.arbitrary), length(movies.50.3$cluster.arbitrary), length(movies.50.4$cluster.arbitrary));
counts
table(arbclusts,counts)
arbclusts = c(1,2,3,4);
counts = c(length(movies.50.1$cluster.arbitrary), length(movies.50.2$cluster.arbitrary), length(movies.50.3$cluster.arbitrary), length(movies.50.4$cluster.arbitrary));
table(arbclusts,counts)
as.data.frame(arbclusts,counts)
as.data.frame(cbind(arbclusts, counts))
membership = c("Not Rated","PG","PG-13","R");
as.data.frame(cbind(arbclusts, membership,counts))
stats::quantile(movies.50, prob=seq(0.1,0.9,by=0.1), type=1 );
stats::quantile(x, prob=seq(0.1,0.9,by=0.1), type=1 );
View(movies.50)
stats::quantile(movies.50$ratings, prob=seq(0.1,0.9,by=0.1), type=1 );
climate = utils::read.csv( paste0(path.mshaffer, "_data_/state-capitals/final/state-capitals-climatedata.txt"), header=TRUE, quote="", sep="|");
##################### WHICH MONTHS #####################
########################################################
months = 1:12; # all the data
#months = c(1,4,7,10); # one month of each of the four seasons
########################################################
month.abb;  # ?month.abb
month.name;
month.name[months];  # these are the names of the months you are selecting ...
# this function would allow us to use different months as criteria and different climate-data keys.  It is variadic and flexible.  `key.n` are the names we will use for our new columns ...
climate.df = buildClimateDataFrame(climate, months, keys=c("Record high F (C)", "Average high F (C)", "Average low F (C)", "Record low F (C)", "Average precipitation inches (mm)", "Average snowfall inches (cm)"), keys.n = c("highmax", "highavg",  "lowavg", "lowmin", "rain", "snow") );
climate.df;
names(climate.df); # this helps you see the indexes ...
##################### WHICH COLUMNS #####################
########################################################
#X = climate.df[,5:52];  # temperature
X = climate.df[,5:76];  # everything (includes rain)
#X = climate.df[,5:20];  # temperature, 1 month per season
#X = climate.df[,5:28];  # everything (includes rain), 1 month per season
########################################################
rownames(X) = climate.df$labels;
Xs = scale(X);
library(Hmisc); # p-values for correlation
low = subsetDataFrame(climate, c("key", "units"), "==", c("Record low F (C)",1));
low = merge(low, capitals, by=c("capital","state"));
low.X = (low[,c(5:18, 21)]); # numeric data
low.X$latitude = as.numeric(low.X$latitude);
low.X$longitude = as.numeric(low.X$longitude);
low.X$population.2019.est = as.numeric(low.X$population.2019.est);
low.cor = round( cor(low.X), digits=2);
# low.cor.p = rcorr(as.matrix(low.X), type="pearson");  # p-values for statistical significance ... # str(low.cor.p);
# examine Jan (idx = 1)
as.data.frame( low.cor ) ; # so it will render nicely in RStudio
low.cor.january = low.cor[,1];
low.cor.january;
